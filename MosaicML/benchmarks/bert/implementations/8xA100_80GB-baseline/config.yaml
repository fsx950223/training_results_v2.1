algorithms:
callbacks:
  speed_monitor:
  lr_monitor:
  mlperf:
    root_folder: /tmp/
    index: 0
    benchmark: bert
    target: 0.72
    metric_name: MaskedAccuracy
    metric_label: bert_pre_training
    exit_at_target: False
    cache_clear_cmd: /sbin/sysctl -w vm.drop_caches=3

model:
  bert:
    pretrained_model_name: bert-large-uncased
    tokenizer_name: bert-large-uncased
    use_pretrained: false
    model_config:
      attention_probs_dropout_prob: 0.0
      hidden_dropout_prob: 0.0

dataloader:
  num_workers: 8
  persistent_workers: true
  pin_memory: true
  prefetch_factor: 2
  timeout: 0

train_batch_size: 448
train_dataset:
  streaming_c4:
    version: 2
    local: /scratch/mds_datasets/c4v2
    remote: /scratch/mds_datasets/c4v2
    shuffle: true
    tokenizer_name: bert-large-uncased
    max_seq_len: 128
    mlm: true
    mlm_probability: 0.15
    group_method: truncate
    split: train
    # drop_last: true

eval_batch_size: 448
eval_interval: 55ba
evaluators:
- eval_dataset:
    streaming_c4:
      version: 2
      local: /scratch/mds_datasets/c4v2
      remote: /scratch/mds_datasets/c4v2
      shuffle: false
      split: val
      tokenizer_name: bert-large-uncased
      max_seq_len: 128
      group_method: truncate
      mlm: true
      mlm_probability: 0.15

  label: bert_pre_training
  metric_names:
  - LanguageCrossEntropy
  - MaskedAccuracy

optimizers:
  decoupled_adamw:
    betas:
    - 0.9
    - 0.98
    eps: 1.0e-06
    lr: 0.0001
    weight_decay: 1.0e-05

grad_accum: 1
grad_clip_norm: -1.0
max_duration: 6000ba
precision: amp
seed: 42

schedulers:
  multistep_with_warmup:
    t_warmup: 80ba
    gamma: 0.9
    milestones:
    - 400ba
    - 600ba
    - 650ba
    - 800ba
    - 850ba
    - 900ba
    - 950ba
    - 1000ba
    - 1100ba
    - 1150ba
    - 1200ba
    - 1225ba
    - 1250ba
    - 1275ba
    - 1300ba
    - 1325ba
    - 1350ba
    - 1375ba
    - 1400ba
    - 1600ba